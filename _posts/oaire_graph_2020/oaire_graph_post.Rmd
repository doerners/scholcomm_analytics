---
title: "Accessing the OpenAIRE Research Graph dumps"
description: | 
  The OpenAIRE Research Graph provides a wide range of metadata
author:
  - name: Najko Jahn 
    url: https://twitter.com/najkoja
    affiliation: State and University Library Göttingen
    affiliation_url: https://www.sub.uni-goettingen.de/
output: distill::distill_article
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = TRUE
)
options(scipen = 999, digits = 2)
knitr::knit_hooks$set(
  inline = function(x) {
    if (is.numeric(x)) {
      return(prettyNum(x, big.mark = ","))
    } else{
      return(x)
    }
  }
)
```

OpenAIRE has collected and interlinked scholarly communication data from various openly available data sources for over ten years. In December 2019, the open science network released the OpenAIRE Research Graph, a big data dump that contains metadata about  more than 110 milion research publications and 10 milion datasets that are linked to open accss full-texts as well as funder information The data dumps can be downloaded from Zenodo.

The OpenAIRE Research Graph offers many data analytics opportunities, but working with it is challenging. One reason is the size of the dump. Although the OpenAIRE Research Graph is already split into several data files, most of them don't fit the memory of an ordinary laptop (mine is a MacBook Retina Early 2015 with 8 GB Ram and s 256 SDD). Another challenge is the format. The dump consists of compressed XML-files, in which only certain elements may be needed for a data analysis.

In this blog post, I introduce the R package openairegraphs, a work-in-progress effort, that helps you to transform large data files from the OpenAIRE Research Graph and to retrieve relevant small data for a data analysis. Focusing on grant-supported research results from the European Commission's Horizon 2020 funding programme (H2020), I present how to subset and analyse the graph using this package. My specific question is how the open access compliance rate of grant-supported projects affiliated with the University of Göttingen compares across the H2020 funding programme.

## What is the R package openairegraphs about?

So far, the R package openairegraphs, which is available on GitHub, contains two sets of functions. The first set provides helpers to split a large OpenAIRE Research Graph data file into seperate, decompressed XML records that can be stored in a dedicated folder on your local computer. The other set consists of parsers that convert data from these XML files to a table-like representation following the tidyverse philosophy, a popular approach and toolset for data analysis with R. Splitting, de-compressing and parsing are essential steps before analysing the OpenAIRE Research Graph.

### Installation 

`openairegraph` can be installed from GitHub using the `remotes` package:

```r
library(remotes)
remotes::install_github("njahn82/openairegraph")
```

### Loading a dump into R

Several dumps from the OpenAIRE Research Graph are available on Zenodo. So far, I developed `openairegraph` to work with the dump `h2020_results.gz`, which comprises research outputs funded by the European Commission's Horizon 2020 funding programme.

After downloading it, the dump `h2020_results.gz` can be imported into R using the jsonlite package. The following exampel shoas that in the json file, each line contains the record identifier and the corresponding Base64-encoded XML record. Base64 is a standard that allows file compression in a text-based format.

```{r}
library(jsonlite) # tools to work with json files
library(tidyverse) # tools from the tidyverse useful for data analysis
oaire <- jsonlite::stream_in(file("data/h2020_results.gz"), verbose = FALSE) %>%
  tibble::as_tibble()
oaire
```

<!--Troughout this blog post, I will use the tidyverse tools to transform and analyse the OpenAIRE Research Graph. The resulting data-frame is therefore represented as a tibble, a central data structure in the tidyverse.-->

### De-compressing and storing OpenAIRE records

The function `openairegraph::oarg_decode`  allows de-coding each record: 

```{r, echo = FALSE}
library(openairegraph)
openairegraph::oarg_decode(oaire, records_path = "data/records/", limit = 500, verbose = FALSE)
```

It writes out each de-coded record to a folder specified by the parameter `records_path`. Because the dumps are quite large, the function furthermore has a parameter that allows setting a limit, which is helpful for inspecting the output first. By default, a progress bar presents the current state of the process.

### Parsing OpenAIRE records

So far, there are four parser available to consume the H2020 results set:

- `openairegraph::oarg_publications_md()` retrieves basic publication metadata complemented by author details and access status
- `openairegraph::oarg_linked_projects()` parses projects related to the publication
- `openairegraph::oarg_linked_ftxt()` gives full-text links including access information
- `openairegraph::oarg_linked_affiliations()` parses affiliation data

These parsers can be used alone, or together like this: 

First, obtain the file paths of the de-coded XML records.

```{r}
openaire_records <- list.files("data/records", full.names = TRUE)
```

Next, we read each XML file using the xml2 package, and apply three parsers: `openairegraph::oarg_publications_md()`, `openairegraph::oarg_linked_projects()` and `openairegraph::oarg_linked_ftxt()`, resulting in a tibble.

```{r}
library(xml2)
oaire_df <- purrr::map_df(openaire_records, function(files) {
  # load xml file
  doc <- xml2::read_xml(files)
  # parser
  out <- oarg_publications_md(doc)
  out$linked_projects <- list(oarg_linked_projects(doc))
  out$linked_ftxt <- list(oarg_linked_ftxt(doc))
  # use file path as id
  out$id <- files
  out
})
oaire_df
```

A note on performance: Parsing the whole dump `h2020_results` took me around 25 minutes on my Mac Book Pro (Early 2015) with 8 GB and SSD flash storage. I therefore recommend to back up the resulting data, instead of un-packing the whole dump for each analysis. Using `jsonlite::stream_out` outputs a text-based json-file, that can be versioned and prevents list-columns.

```{r}
jsonlite::stream_out(oaire_df, file("data/h2020_parsed.json"))
```

## Real-World Example: Monitoring the Open Access Compliance across H2020 grant-supported projects at the institutional-level

Usually, individual researchers do not sign grant agreements with the European Commission (EC), but the institution they are affiliated with. Universities and other research institutions administering EC-funded projects are therefore looking for ways to monitor the compliance with funder rules. Often, librarians will be assigned this task. Here, we will illustrate how an institution can use data from the OpenAIRE Research Graph to benchmark compliance with the HORIZON 2020 open access mandate relative to the overall performance of the different H2020 funding programmes.

As a start, we loaded a dataset, which was compiled following the above-described methods using the whole `h2020_results` dump.

```{r}
oaire_df <- jsonlite::stream_in(file("data/h2020_parsed.json"), verbose = FALSE) %>%
  tibble::as_tibble()
```

Our aim is to analyse open access uptake levels across H2020 projects affiliated with the University of Göttingen relative to overall performance. As a first step, we choose project and access information from the data. Because publications can result from more than one project, funding information is stored in a nested data frame.

```{r}
pubs_projects <- oaire_df %>%
  select(id, type, best_access_right, linked_projects) %>%
  unnest(linked_projects) 
pubs_projects
```

Next, we want to identy those H2020 projects where the University of Göttingen participates in. There at least two ways to obtain more detailed project information: One is the OpenAIRE Reasearch Graph providing detailed information in another large dump, the other to merge with [open data provided by the European Commission](https://data.europa.eu/euodp/en/data/dataset/cordisH2020projects). For convenience, we will use the second option.

```{r}
# load local copy downloaded from the EC open data portal
cordis_org <- readr::read_delim("data/cordis-h2020organizations .csv", delim = ";",
                               locale = locale(decimal_mark = ",")) %>%
  # data cleaning
  mutate_if(is.double, as.character) 
```

Identify projects with Uni Göttingen participation

```{r}
ugoe_projects <- cordis_org %>%
  filter(shortName %in% c("UGOE", "UMG-GOE")) %>% 
  select(project_id = projectID, role, project_acronym = projectAcronym)
```

Add it to our projects list

```{r}
pubs_projects_ugoe <- pubs_projects %>%
  mutate(ugoe_project = funding_stream == "H2020" & project_code %in% ugoe_projects$project_id)
```


## Conclussion

